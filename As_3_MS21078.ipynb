{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementing A Multi-layered Perceptron Using Numpy**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YzeSV5aFgmOO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlC-5x_CwbKK"
      },
      "source": [
        "#Problem 1:\n",
        "\n",
        "Here you must read an input file. Each line contains 785 numbers (comma delimited): the first number in each row denotes the class label: 0 corresponds to digit 0, 1 corresponds to digit 1, etc. The rest of the values are the 784 pixel values between 0 and 255 correspondig to black and white images.  As a warm up  question, load the data.\n",
        "\n",
        "For this problem you must write a function that takes a file path as an argument which contains  this data. Your function must return two values (x and y) that contains the data from the file as  described. Specifically, the first return value (x) must be a matrix where the rows are individual  examples of images, and the columns are individual pixels (n x 784 matrix). The second return value  must be a list/array of real numbers representing the labels of the examples (rows) in x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg6y4Tj1wT_7"
      },
      "outputs": [],
      "source": [
        "#Packages\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5Ap0Uh4hdln",
        "outputId": "b56baf68-9a11-4257-c6a9-14f5c3ac77c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 18.0, 18.0, 18.0, 126.0, 136.0, 175.0, 26.0, 166.0, 255.0, 247.0, 127.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 36.0, 94.0, 154.0, 170.0, 253.0, 253.0, 253.0, 253.0, 253.0, 225.0, 172.0, 253.0, 242.0, 195.0, 64.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.0, 238.0, 253.0, 253.0, 253.0, 253.0, 253.0, 253.0, 253.0, 253.0, 251.0, 93.0, 82.0, 82.0, 56.0, 39.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 18.0, 219.0, 253.0, 253.0, 253.0, 253.0, 253.0, 198.0, 182.0, 247.0, 241.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 80.0, 156.0, 107.0, 253.0, 253.0, 205.0, 11.0, 0.0, 43.0, 154.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 1.0, 154.0, 253.0, 90.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 139.0, 253.0, 190.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 190.0, 253.0, 70.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 35.0, 241.0, 225.0, 160.0, 108.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 81.0, 240.0, 253.0, 253.0, 119.0, 25.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 45.0, 186.0, 253.0, 253.0, 150.0, 27.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 93.0, 252.0, 253.0, 187.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 249.0, 253.0, 249.0, 64.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 46.0, 130.0, 183.0, 253.0, 253.0, 207.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.0, 148.0, 229.0, 253.0, 253.0, 253.0, 250.0, 182.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.0, 114.0, 221.0, 253.0, 253.0, 253.0, 253.0, 201.0, 78.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 23.0, 66.0, 213.0, 253.0, 253.0, 253.0, 253.0, 198.0, 81.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 18.0, 171.0, 219.0, 253.0, 253.0, 253.0, 253.0, 195.0, 80.0, 9.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 55.0, 172.0, 226.0, 253.0, 253.0, 253.0, 253.0, 244.0, 133.0, 11.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 136.0, 253.0, 253.0, 253.0, 212.0, 135.0, 132.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 51.0, 159.0, 253.0, 159.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 48.0, 238.0, 252.0, 252.0, 252.0, 237.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 54.0, 227.0, 253.0, 252.0, 239.0, 233.0, 252.0, 57.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 60.0, 224.0, 252.0, 253.0, 252.0, 202.0, 84.0, 252.0, 253.0, 122.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 163.0, 252.0, 252.0, 252.0, 253.0, 252.0, 252.0, 96.0, 189.0, 253.0, 167.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 51.0, 238.0, 253.0, 253.0, 190.0, 114.0, 253.0, 228.0, 47.0, 79.0, 255.0, 168.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 48.0, 238.0, 252.0, 252.0, 179.0, 12.0, 75.0, 121.0, 21.0, 0.0, 0.0, 253.0, 243.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 38.0, 165.0, 253.0, 233.0, 208.0, 84.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 253.0, 252.0, 165.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 178.0, 252.0, 240.0, 71.0, 19.0, 28.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 253.0, 252.0, 195.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 57.0, 252.0, 252.0, 63.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 253.0, 252.0, 195.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 198.0, 253.0, 190.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 255.0, 253.0, 196.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 76.0, 246.0, 252.0, 112.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 253.0, 252.0, 148.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 252.0, 230.0, 25.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 135.0, 253.0, 186.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 252.0, 223.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 131.0, 252.0, 225.0, 71.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 252.0, 145.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 48.0, 165.0, 252.0, 173.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 86.0, 253.0, 225.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 114.0, 238.0, 253.0, 162.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 252.0, 249.0, 146.0, 48.0, 29.0, 85.0, 178.0, 225.0, 253.0, 223.0, 167.0, 56.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 252.0, 252.0, 252.0, 229.0, 215.0, 252.0, 252.0, 252.0, 196.0, 130.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.0, 199.0, 252.0, 252.0, 253.0, 252.0, 252.0, 233.0, 145.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 25.0, 128.0, 252.0, 253.0, 252.0, 141.0, 37.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "y: [5, 0]\n"
          ]
        }
      ],
      "source": [
        "def read_image_data(file_path):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        next(file)\n",
        "        for line in file:\n",
        "            values = line.strip().split(',')  # Split\n",
        "            label = int(values[0])\n",
        "            pixels = [float(val) for val in values[1:]]\n",
        "            x.append(pixels)\n",
        "            y.append(label)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "#mnist dataset is stored in my Gdrive\n",
        "x, y = read_image_data('/content/drive/MyDrive/mnist_train.csv')\n",
        "print(\"x:\", x[:2])  # Print first two rows of x\n",
        "print(\"y:\", y[:2])  # Print first two elements of y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmEG0d2WytJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "594fb980-7d8d-4517-be51-2015c259ebcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label for the image is  4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Understanding the data\n",
        "import matplotlib.pyplot as plt\n",
        "i=6424\n",
        "img=np.array(x[i])\n",
        "img=img.reshape(28,28)\n",
        "plt.imshow(img)\n",
        "print(\"Label for the image is \",y[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFQAqeP02PlG"
      },
      "source": [
        "#Problem 2:\n",
        "\n",
        "Implement the backpropagation algorithm in a zero hidden layer neural network (weights between input and output nodes). The output layer should be a softmax output over 10 classes corresponding to 10 classes of handwritten digits (e.g. An architecture: 784 > 10). Your backprop code should minimize the cross-entropy function for multi-class classification problems (categorical  cross entropy).\n",
        "\n",
        " $\\text{Loss} = - \\sum_j \\text{target}_j \\cdot \\log(\\text{prediction}_j)$\n",
        "\n",
        "\n",
        "where j is the class label\n",
        "\n",
        "This step should be done with a full step of gradient descent, not stochastic gradient descent or rmsprop. For this  problem you must write a function that takes as an input a matrix of x values, a list of y values (as  returned from problem 1), a weight matrix, and a learning rate and performs a single step of  backpropagation. You will need to do both a forward step with the inputs, and then a backward prop to  get the gradients. Return the updated weight matrix and bias in the same format as it was passed.\n",
        "\n",
        "The list of weight matrices will be a list with 1 entry where the only entry is a matrix in the  format where the rows represent all of the outgoing weights for a neuron in the input layer and the  columns represent the weights for the incoming neurons. A specific row column index will give you the  weight for a neuron to neuron connection.\n",
        "\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same  length as the first set of weights. (e.g. For an architecture of 784 > 10, there will be a single element list  with a vector of size 10)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network1(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        z = np.dot(self.weights[0], a) + self.biases[0]\n",
        "        return softmax(z)\n",
        "\n",
        "    def gradient_descent1(self, x, y, learning_rate):\n",
        "        # Forward pass\n",
        "        z = np.dot(self.weights[0], x) + self.biases[0]\n",
        "        activation = softmax(z)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = activation - y\n",
        "\n",
        "        nabla_b = delta\n",
        "        nabla_w = np.dot(delta, x.transpose())\n",
        "\n",
        "        # Gradient descent update\n",
        "        self.weights[0] -= learning_rate * nabla_w\n",
        "        self.biases[0] -= learning_rate * nabla_b\n",
        "\n",
        "def softmax(z):\n",
        "    max_z = np.max(z)\n",
        "    exp_scores = np.exp(z - max_z)  # Softmax normalization\n",
        "    return exp_scores / np.sum(exp_scores)"
      ],
      "metadata": {
        "id": "_zch0Feo1B7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Packages for training and accuracy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "8uoo1RqKJooj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the mnist dataset from google drive\n",
        "x, y = read_image_data('/content/drive/MyDrive/mnist_train.csv')\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "#(80% training, 20% test)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "tVbAdPyI0RV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net1 = Network1([784, 10])\n",
        "\n",
        "# Train the model\n",
        "def train_model(net1, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((net1.sizes[-1], 1))\n",
        "            y_one_hot[y] = 1\n",
        "            net1.gradient_descent1(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "#hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "\n",
        "# Train the model\n",
        "train_model(net1, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(net, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        output = net.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "y_pred = evaluate_model(net1, x_test, y_test)\n",
        "\n",
        "#accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5luHjpq1kDZ",
        "outputId": "90a6e448-b3e1-4a94-8650-aa888ea10a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.86675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es4-erMf3ChO"
      },
      "source": [
        "#Problem 3:\n",
        "\n",
        "Extend your code from problem 2 to support a single layer neural network with n hidden units (e.g. An  architecture: 784 > 10 > 10). These hidden units should be using sigmoid activations.\n",
        "\n",
        "For this problem you must write a function that takes as an input a matrix of x values, a list of y  values (as returned from problem 1), list of weight matrices, a list of bias vectors, and a learning rate and performs a single step of backpropagation. You will need to do both a forward step with the inputs to get the outputs, and then a backward prop to get the gradients. Return the  updated weight matrix and bias in the same format as it was passed.\n",
        "\n",
        "The list of weight matrices is a list with 2 entries where each entry in the list contains a single weight matrix as previously defined in problem 2. For a network with shape 784 > 10 > 10 the passed list  of weight matrices would look like this: [matrix with shape 784x10, matrix with shape 10x10]. Note:  though a hidden layer of size 10 is used as an example here, your code must be able to support a hidden  layer of dimension n.\n",
        "\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same  length as the first set of weights. (e.g. For an architecture of 784 > 10 > 10, there will be a two element  list with an vector of size 10 and a vector of size 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network2(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])] #xavier initialization\n",
        "\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    def gradient_descent(self, x, y, learning_rate):\n",
        "        # Forward pass\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activations[-1]) + b\n",
        "            zs.append(z)\n",
        "            activations.append(sigmoid(z))\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b = [delta]\n",
        "        nabla_w = [np.dot(delta, activations[-2].transpose())]\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
        "            nabla_b.append(delta)\n",
        "            nabla_w.append(np.dot(delta, activations[-l - 1].transpose()))\n",
        "        nabla_b.reverse()\n",
        "        nabla_w.reverse()\n",
        "\n",
        "        # Gradient descent update\n",
        "        self.weights = [w - learning_rate * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - learning_rate * nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return output_activations - y\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n"
      ],
      "metadata": {
        "id": "CTuHPiMSBqeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Network2([784,10,10])"
      ],
      "metadata": {
        "id": "od3ASArjLuc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "x, y = read_image_data('/content/drive/MyDrive/mnist_train.csv')\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "# (80% training, 20% test)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x_train = (x_train - np.mean(x_train)) / np.std(x_train)\n",
        "x_test = (x_test - np.mean(x_test)) / np.std(x_test)"
      ],
      "metadata": {
        "id": "aj2z1hxUL3Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def train_model(net2, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "\n",
        "            # Perform gradient descent update\n",
        "            net2.gradient_descent(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "\n",
        "# Train the model\n",
        "train_model(net2, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(net2, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = net2.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "y_pred = evaluate_model(net2, x_test, y_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3IWfcLhLqel",
        "outputId": "a55dc089-0cb7-4720-84f2-3dd88c6eb6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8661666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   From problem 3 and 4, Experimenting with Xavier intialization {self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])]} helped neural netwrok with n hidden layers to increase the accuracy at some extent.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0AYlDKXzAA_r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT77_c2Q3IKw"
      },
      "source": [
        "#Problem 4:\n",
        "\n",
        "Extend your code from problem 3 (use cross entropy error) and implement a multi-layer neural  network, starting with a simple architecture containing any number of hidden units in each layer (e.g. With  architecture: 784 > 10 > 10 > 10). These hidden units should be using sigmoid activations.\n",
        "\n",
        "For this problem you must write a function that takes as an input a matrix of x values, a list of y  values (as returned from problem 1), list of weight matrices, a list of bias vectors, and a learning rate and  performs a single step of backpropagation. You will need to do both a forward step with the inputs to  get the outputs, and then a backward prop to get the gradients. Return the updated weight matrix and  bias in the same format as it was passed.\n",
        "\n",
        "The list of weight matrices is a list with k entries where each entry in the list contains a single  weight matrix as previously defined in problem 2. For a network with shape 784 > 10 > 10 > 10 the  passed list of weight matrices would look like this: [matrix with shape 784x10, matrix with shape 10x10,  matrix with shape 10x10]. Note: though a hidden layer of size 10 is used as an example here, your code  must be able to support a hidden layer of dimension n.\n",
        "\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same  length as the first set of weights. (e.g. For an architecture of 784 > 10 > 10, there will be a two element  list with an vector of size 10 and a vector of size 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzxogZ2z3NJ8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Network3(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        #self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])] #xavier initialization\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    def gradient_descent3(self, x, y, learning_rate):\n",
        "        # Forward pass\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y)\n",
        "        nabla_b = [np.sum(delta, axis=1, keepdims=True)]\n",
        "        nabla_w = [np.dot(delta, activations[-2].transpose())]\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
        "            nabla_b.insert(0, np.sum(delta, axis=1, keepdims=True))\n",
        "            nabla_w.insert(0, np.dot(delta, activations[-l - 1].transpose()))\n",
        "\n",
        "        # Gradient descent update\n",
        "        self.weights = [w - learning_rate * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - learning_rate * nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations - y)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net3 = Network3([784,10,10,10])"
      ],
      "metadata": {
        "id": "Amf91fH1s9TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "x, y = read_image_data('/content/drive/MyDrive/mnist_train.csv')\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the data into training and test sets (80% training, 20% test)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x_train = (x_train - np.mean(x_train)) / np.std(x_train)\n",
        "x_test = (x_test - np.mean(x_test)) / np.std(x_test)"
      ],
      "metadata": {
        "id": "nr1ARIIotGcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def train_model(net3, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "\n",
        "            net3.gradient_descent3(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "\n",
        "# Train the model\n",
        "train_model(net3, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(net3, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = net3.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "y_pred = evaluate_model(net3, x_test, y_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FXxE9xStLV2",
        "outputId": "cbf365ae-c7d5-49a7-f73a-1db50bea3ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9066666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Multi-layered neural network and neural network with n hidden layers shows an increased accuracy while performing xavier initialization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q2tr6hVtB1Kz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GJyBnr3Nhd"
      },
      "source": [
        "#Problem 5:\n",
        "\n",
        "Extend your code from problem 4 to implement different activations functions which will be  passed as a parameter. In this problem all activations (except the final layer which should remain a  softmax) must be changed to the passed activation function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network4(object):\n",
        "    def __init__(self, sizes, activation_function):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])] # Xavier initialization\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
        "            a = self.activation_function(np.dot(w, a) + b)\n",
        "        z_output = np.dot(self.weights[-1], a) + self.biases[-1]\n",
        "        output = softmax(z_output)\n",
        "        return output\n",
        "\n",
        "    def gradient_descent4(self, x, y, learning_rate):\n",
        "        # Forward pass\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = self.activation_function(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y)\n",
        "        nabla_b = [np.sum(delta, axis=1, keepdims=True)]\n",
        "        nabla_w = [np.dot(delta, activations[-2].transpose())]\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = self.activation_prime(z)\n",
        "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
        "            nabla_b.insert(0, np.sum(delta, axis=1, keepdims=True))\n",
        "            nabla_w.insert(0, np.dot(delta, activations[-l - 1].transpose()))\n",
        "\n",
        "        # Gradient descent update\n",
        "        self.weights = [w - learning_rate * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - learning_rate * nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations - y)\n",
        "\n",
        "    def activation_prime(self, z):\n",
        "        if self.activation_function == sigmoid:\n",
        "            return sigmoid_prime(z)\n",
        "        elif self.activation_function == relu:\n",
        "            return relu_prime(z)\n",
        "        elif self.activation_function == tanh:\n",
        "            return tanh_prime(z)\n",
        "        elif self.activation_function == leaky_relu:\n",
        "            return leaky_relu_prime(z)\n",
        "\n",
        "# Define activation functions\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_prime(z):\n",
        "    return np.where(z > 0, 1, 0)\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def tanh_prime(z):\n",
        "    return 1 - np.tanh(z)**2\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.where(z > 0, z, z * alpha)\n",
        "\n",
        "def leaky_relu_prime(z, alpha=0.01):\n",
        "    return np.where(z > 0, 1, alpha)\n",
        "\n",
        "def softmax(z):\n",
        "    max_z = np.max(z)\n",
        "    exp_scores = np.exp(z - max_z)\n",
        "    return exp_scores / np.sum(exp_scores)\n"
      ],
      "metadata": {
        "id": "9er8tJd8jOK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Here it is necessary to use Xavier initialization to get a proper accuracy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iuLrVZusEmkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Leaky ReLu"
      ],
      "metadata": {
        "id": "COKQvhWsjv0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [784, 10, 10, 10]\n",
        "activation_function = leaky_relu\n",
        "netlr = Network4(sizes, activation_function)"
      ],
      "metadata": {
        "id": "-1bEefASjsnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "def train_model(netlr, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "            netlr.gradient_descent4(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "#hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "\n",
        "# Train the model\n",
        "train_model(netlr, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(netlr, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = netlr.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "y_pred = evaluate_model(netlr, x_test, y_test)\n",
        "\n",
        "# accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGUyaE0dju-s",
        "outputId": "8059d987-f317-44db-c448-0ffb5b13cee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9200833333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sigmoid\n"
      ],
      "metadata": {
        "id": "OxhK0WSuVVro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [784, 10, 10, 10]\n",
        "activation_function = sigmoid\n",
        "nets = Network4(sizes, activation_function)"
      ],
      "metadata": {
        "id": "uG5zld3oRlEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "def train_model(nets, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "            nets.gradient_descent4(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "\n",
        "# Train the model\n",
        "train_model(nets, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(nets, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = nets.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "y_pred = evaluate_model(nets, x_test, y_test)\n",
        "\n",
        "# accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac87f121-bfc4-4347-c271-0806f1d2d94a",
        "id": "x8gFoms6Rbga"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ReLu\n"
      ],
      "metadata": {
        "id": "o1mXC1a_Vqj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [784, 10, 10, 10]\n",
        "activation_function = relu\n",
        "netr = Network4(sizes, activation_function)"
      ],
      "metadata": {
        "id": "iR0WcibtVqAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "def train_model(netr, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "            netr.gradient_descent4(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "\n",
        "# Train the model\n",
        "train_model(netr, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(netr, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = netr.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "y_pred = evaluate_model(netr, x_test, y_test)\n",
        "\n",
        "# accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gAJgAWXVzYQ",
        "outputId": "499ad118-6b50-480b-80b4-83fd4978d0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8941666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tanh"
      ],
      "metadata": {
        "id": "4P8LTdStWB-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [784, 10, 10, 10]\n",
        "activation_function = tanh\n",
        "nett = Network4(sizes, activation_function)"
      ],
      "metadata": {
        "id": "F-p-XPc7WDsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "def train_model(nett, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "            nett.gradient_descent4(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "\n",
        "# Train the model\n",
        "train_model(nett, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(nett, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = nett.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "y_pred = evaluate_model(nett, x_test, y_test)\n",
        "\n",
        "# accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFigkEGAWF2e",
        "outputId": "80a94890-8cd1-4d96-f84d-2d76c7dd9397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7325833333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   In problem 5, it is better to use Leaky ReLu. It gave accuracy of around 92% which is greter than sigmoid, ReLu and tanh\n",
        "\n"
      ],
      "metadata": {
        "id": "qY0c6FGSFby4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klegSin73SBF"
      },
      "source": [
        "#Problem 6:\n",
        "\n",
        "Extend your code from problem 5 to implement momentum with your gradient descent. The  momentum value will be passed as a parameter. Your function should perform epoch number of  epochs and return the resulting weights."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkM(object):\n",
        "    def __init__(self, sizes, activation_function):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])] # Xavier initialization\n",
        "        self.activation_function = activation_function\n",
        "        self.prev_delta_w = [np.zeros((y, x)) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "        self.prev_delta_b = [np.zeros((y, 1)) for y in sizes[1:]]\n",
        "\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
        "            a = self.activation_function(np.dot(w, a) + b)\n",
        "        z_output = np.dot(self.weights[-1], a) + self.biases[-1]\n",
        "        output = softmax(z_output)\n",
        "        return output\n",
        "\n",
        "    def gradient_descentM(self, x, y, learning_rate, momentum):\n",
        "        # Forward pass\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = self.activation_function(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y)\n",
        "        nabla_b = [np.sum(delta, axis=1, keepdims=True)]\n",
        "        nabla_w = [np.dot(delta, activations[-2].transpose())]\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = self.activation_prime(z)\n",
        "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
        "            nabla_b.insert(0, np.sum(delta, axis=1, keepdims=True))\n",
        "            nabla_w.insert(0, np.dot(delta, activations[-l - 1].transpose()))\n",
        "\n",
        "        # Gradient descent update with momentum\n",
        "        self.weights = [w - learning_rate * nw - momentum * pdw for w, nw, pdw in zip(self.weights, nabla_w, self.prev_delta_w)]\n",
        "        self.biases = [b - learning_rate * nb - momentum * pdb for b, nb, pdb in zip(self.biases, nabla_b, self.prev_delta_b)]\n",
        "        self.prev_delta_w = nabla_w\n",
        "        self.prev_delta_b = nabla_b\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations - y)\n",
        "\n",
        "    def activation_prime(self, z):\n",
        "        if self.activation_function == sigmoid:\n",
        "            return sigmoid_prime(z)\n",
        "        elif self.activation_function == relu:\n",
        "            return relu_prime(z)\n",
        "        elif self.activation_function == tanh:\n",
        "            return tanh_prime(z)\n",
        "        elif self.activation_function == leaky_relu:\n",
        "            return leaky_relu_prime(z)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-np.clip(z, -709, 709)))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_prime(z):\n",
        "    return np.where(z > 0, 1, 0)\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def tanh_prime(z):\n",
        "    return 1 - np.tanh(z)**2\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.where(z > 0, z, z * alpha)\n",
        "\n",
        "def leaky_relu_prime(z, alpha=0.01):\n",
        "    return np.where(z > 0, 1, alpha)\n",
        "\n",
        "def softmax(z):\n",
        "    max_z = np.max(z)\n",
        "    exp_scores = np.exp(z - max_z)  # Softmax normalization\n",
        "    return exp_scores / np.sum(exp_scores)\n"
      ],
      "metadata": {
        "id": "qgCR7BGZSvBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [784, 10, 10, 10]\n",
        "activation_function = sigmoid\n",
        "netM = NetworkM(sizes, activation_function)\n",
        "\n",
        "# Train the model with momentum\n",
        "def train_model_with_momentum(netM, x_train, y_train, learning_rate, momentum, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "\n",
        "\n",
        "            netM.gradient_descentM(x.reshape(-1, 1), y_one_hot, learning_rate, momentum)\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = .0001\n",
        "momentum = 0.005\n",
        "epochs = 10\n",
        "\n",
        "# Train the model\n",
        "train_model_with_momentum(netM, x_train, y_train, learning_rate, momentum, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "y_pred = evaluate_model(netM, x_test, y_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "QimDqrXIS9RA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684a458b-c026-410f-90f9-64958dceb6d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9119166666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   For Learning_rate=0.01, momentum =0.1 and epochs=10: Accuracy = 85%\n",
        "*   learning_rate = 0.001, momentum = 0.01 and epochs = 10: Accuracy = 90%\n",
        "*   learning_rate = 0.0001, momentum = 0.01 and epochs = 10: Accuracy = 90.4%\n",
        "*   learning_rate = 0.0001, momentum = 0.001 and epochs = 10: Accuracy = 90.2%\n",
        "*   learning_rate = 0.001, momentum = 0.2 and epochs = 10: Accuracy = 82.8%\n",
        "*   learning_rate = 0.001, momentum = 0.05 and epochs = 10: Accuracy = 90.2%\n",
        "*   learning_rate = 0.0001, momentum = 0.005 and epochs = 10: Accuracy = 91.2%\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vg-VDX4AHePi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}