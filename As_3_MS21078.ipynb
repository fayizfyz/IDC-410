{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementing A Multi-layered Perceptron Using Numpy**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YzeSV5aFgmOO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlC-5x_CwbKK"
      },
      "source": [
        "#Problem 1:\n",
        "\n",
        "Here you must read an input file. Each line contains 785 numbers (comma delimited): the first number in each row denotes the class label: 0 corresponds to digit 0, 1 corresponds to digit 1, etc. The rest of the values are the 784 pixel values between 0 and 255 correspondig to black and white images.  As a warm up  question, load the data.\n",
        "\n",
        "For this problem you must write a function that takes a file path as an argument which contains  this data. Your function must return two values (x and y) that contains the data from the file as  described. Specifically, the first return value (x) must be a matrix where the rows are individual  examples of images, and the columns are individual pixels (n x 784 matrix). The second return value  must be a list/array of real numbers representing the labels of the examples (rows) in x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wg6y4Tj1wT_7"
      },
      "outputs": [],
      "source": [
        "#Packages\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5Ap0Uh4hdln",
        "outputId": "461364ad-c837-4961-a75c-1b6a8627803a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 18.0, 18.0, 18.0, 126.0, 136.0, 175.0, 26.0, 166.0, 255.0, 247.0, 127.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 36.0, 94.0, 154.0, 170.0, 253.0, 253.0, 253.0, 253.0, 253.0, 225.0, 172.0, 253.0, 242.0, 195.0, 64.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.0, 238.0, 253.0, 253.0, 253.0, 253.0, 253.0, 253.0, 253.0, 253.0, 251.0, 93.0, 82.0, 82.0, 56.0, 39.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 18.0, 219.0, 253.0, 253.0, 253.0, 253.0, 253.0, 198.0, 182.0, 247.0, 241.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 80.0, 156.0, 107.0, 253.0, 253.0, 205.0, 11.0, 0.0, 43.0, 154.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.0, 1.0, 154.0, 253.0, 90.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 139.0, 253.0, 190.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 190.0, 253.0, 70.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 35.0, 241.0, 225.0, 160.0, 108.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 81.0, 240.0, 253.0, 253.0, 119.0, 25.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 45.0, 186.0, 253.0, 253.0, 150.0, 27.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 93.0, 252.0, 253.0, 187.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 249.0, 253.0, 249.0, 64.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 46.0, 130.0, 183.0, 253.0, 253.0, 207.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.0, 148.0, 229.0, 253.0, 253.0, 253.0, 250.0, 182.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.0, 114.0, 221.0, 253.0, 253.0, 253.0, 253.0, 201.0, 78.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 23.0, 66.0, 213.0, 253.0, 253.0, 253.0, 253.0, 198.0, 81.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 18.0, 171.0, 219.0, 253.0, 253.0, 253.0, 253.0, 195.0, 80.0, 9.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 55.0, 172.0, 226.0, 253.0, 253.0, 253.0, 253.0, 244.0, 133.0, 11.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 136.0, 253.0, 253.0, 253.0, 212.0, 135.0, 132.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 51.0, 159.0, 253.0, 159.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 48.0, 238.0, 252.0, 252.0, 252.0, 237.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 54.0, 227.0, 253.0, 252.0, 239.0, 233.0, 252.0, 57.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 60.0, 224.0, 252.0, 253.0, 252.0, 202.0, 84.0, 252.0, 253.0, 122.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 163.0, 252.0, 252.0, 252.0, 253.0, 252.0, 252.0, 96.0, 189.0, 253.0, 167.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 51.0, 238.0, 253.0, 253.0, 190.0, 114.0, 253.0, 228.0, 47.0, 79.0, 255.0, 168.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 48.0, 238.0, 252.0, 252.0, 179.0, 12.0, 75.0, 121.0, 21.0, 0.0, 0.0, 253.0, 243.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 38.0, 165.0, 253.0, 233.0, 208.0, 84.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 253.0, 252.0, 165.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 178.0, 252.0, 240.0, 71.0, 19.0, 28.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 253.0, 252.0, 195.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 57.0, 252.0, 252.0, 63.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 253.0, 252.0, 195.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 198.0, 253.0, 190.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 255.0, 253.0, 196.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 76.0, 246.0, 252.0, 112.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 253.0, 252.0, 148.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 252.0, 230.0, 25.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 135.0, 253.0, 186.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 252.0, 223.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 131.0, 252.0, 225.0, 71.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 252.0, 145.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 48.0, 165.0, 252.0, 173.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 86.0, 253.0, 225.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 114.0, 238.0, 253.0, 162.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 252.0, 249.0, 146.0, 48.0, 29.0, 85.0, 178.0, 225.0, 253.0, 223.0, 167.0, 56.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 252.0, 252.0, 252.0, 229.0, 215.0, 252.0, 252.0, 252.0, 196.0, 130.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.0, 199.0, 252.0, 252.0, 253.0, 252.0, 252.0, 233.0, 145.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 25.0, 128.0, 252.0, 253.0, 252.0, 141.0, 37.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "y: [5, 0]\n"
          ]
        }
      ],
      "source": [
        "def read_image_data(file_path):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        next(file)\n",
        "        for line in file:\n",
        "            values = line.strip().split(',')  # Split\n",
        "            label = int(values[0])\n",
        "            pixels = [float(val) for val in values[1:]]\n",
        "            x.append(pixels)\n",
        "            y.append(label)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "#mnist dataset is stored in my Gdrive\n",
        "x, y = read_image_data('/content/drive/MyDrive/mnist_train.csv')\n",
        "print(\"x:\", x[:2])\n",
        "print(\"y:\", y[:2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VmEG0d2WytJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "00680359-f478-4e6d-d287-53d6d1019243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label for the image is  4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Understanding the data\n",
        "import matplotlib.pyplot as plt\n",
        "i=6424\n",
        "img=np.array(x[i])\n",
        "img=img.reshape(28,28)\n",
        "plt.imshow(img)\n",
        "print(\"Label for the image is \",y[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFQAqeP02PlG"
      },
      "source": [
        "#Problem 2:\n",
        "\n",
        "Implement the backpropagation algorithm in a zero hidden layer neural network (weights between input and output nodes). The output layer should be a softmax output over 10 classes corresponding to 10 classes of handwritten digits (e.g. An architecture: 784 > 10). Your backprop code should minimize the cross-entropy function for multi-class classification problems (categorical  cross entropy).\n",
        "\n",
        " $\\text{Loss} = - \\sum_j \\text{target}_j \\cdot \\log(\\text{prediction}_j)$\n",
        "\n",
        "\n",
        "where j is the class label\n",
        "\n",
        "This step should be done with a full step of gradient descent, not stochastic gradient descent or rmsprop. For this  problem you must write a function that takes as an input a matrix of x values, a list of y values (as  returned from problem 1), a weight matrix, and a learning rate and performs a single step of  backpropagation. You will need to do both a forward step with the inputs, and then a backward prop to  get the gradients. Return the updated weight matrix and bias in the same format as it was passed.\n",
        "\n",
        "The list of weight matrices will be a list with 1 entry where the only entry is a matrix in the  format where the rows represent all of the outgoing weights for a neuron in the input layer and the  columns represent the weights for the incoming neurons. A specific row column index will give you the  weight for a neuron to neuron connection.\n",
        "\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same  length as the first set of weights. (e.g. For an architecture of 784 > 10, there will be a single element list  with a vector of size 10)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network1(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        z = np.dot(self.weights[0], a) + self.biases[0]\n",
        "        return softmax(z)\n",
        "\n",
        "    def gradient_descent1(self, x, y, learning_rate):\n",
        "        # Forward pass\n",
        "        z = np.dot(self.weights[0], x) + self.biases[0]\n",
        "        activation = softmax(z)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = activation - y\n",
        "\n",
        "        nabla_b = delta\n",
        "        nabla_w = np.dot(delta, x.transpose())\n",
        "\n",
        "        # Gradient descent update\n",
        "        self.weights[0] -= learning_rate * nabla_w\n",
        "        self.biases[0] -= learning_rate * nabla_b\n",
        "        # Compute and return loss\n",
        "        loss = -np.sum(y * np.log(activation))\n",
        "        return loss\n",
        "\n",
        "def softmax(z):\n",
        "    max_z = np.max(z)\n",
        "    exp_scores = np.exp(z - max_z)  # Softmax normalization\n",
        "    return exp_scores / np.sum(exp_scores)"
      ],
      "metadata": {
        "id": "_zch0Feo1B7D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Packages for training and accuracy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "8uoo1RqKJooj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the mnist dataset from google drive\n",
        "x, y = read_image_data('/content/drive/MyDrive/mnist_train.csv')\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "#(80% training, 20% test)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x_train = (x_train - np.mean(x_train)) / np.std(x_train)\n",
        "x_test = (x_test - np.mean(x_test)) / np.std(x_test)"
      ],
      "metadata": {
        "id": "tVbAdPyI0RV2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net1 = Network1([784, 10])\n",
        "\n",
        "# Train the model\n",
        "\n",
        "def train_model(net1, x_train, y_train, learning_rate, epochs):\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((net1.sizes[-1], 1))\n",
        "            y_one_hot[y] = 1\n",
        "            loss = net1.gradient_descent1(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "            epoch_loss += loss\n",
        "        epoch_loss /= len(x_train)  # Compute average loss for the epoch\n",
        "        losses.append(epoch_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Plot loss curve\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss Curve')\n",
        "    plt.show()\n",
        "#hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "train_model(net1, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(net, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        output = net.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "y_pred = evaluate_model(net1, x_test, y_test)\n",
        "\n",
        "#accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n5luHjpq1kDZ",
        "outputId": "c269e94c-28a5-4db3-ba4e-b1573dd4fd32"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 2.3857\n",
            "Epoch 2/100, Loss: 1.3297\n",
            "Epoch 3/100, Loss: 1.1096\n",
            "Epoch 4/100, Loss: 0.9944\n",
            "Epoch 5/100, Loss: 0.9261\n",
            "Epoch 6/100, Loss: 0.8767\n",
            "Epoch 7/100, Loss: 0.8436\n",
            "Epoch 8/100, Loss: 0.8134\n",
            "Epoch 9/100, Loss: 0.7968\n",
            "Epoch 10/100, Loss: 0.7771\n",
            "Epoch 11/100, Loss: 0.7668\n",
            "Epoch 12/100, Loss: 0.7546\n",
            "Epoch 13/100, Loss: 0.7449\n",
            "Epoch 14/100, Loss: 0.7431\n",
            "Epoch 15/100, Loss: 0.7347\n",
            "Epoch 16/100, Loss: 0.7271\n",
            "Epoch 17/100, Loss: 0.7257\n",
            "Epoch 18/100, Loss: 0.7179\n",
            "Epoch 19/100, Loss: 0.7161\n",
            "Epoch 20/100, Loss: 0.7118\n",
            "Epoch 21/100, Loss: 0.7144\n",
            "Epoch 22/100, Loss: 0.7055\n",
            "Epoch 23/100, Loss: 0.7060\n",
            "Epoch 24/100, Loss: 0.7037\n",
            "Epoch 25/100, Loss: 0.7033\n",
            "Epoch 26/100, Loss: 0.6980\n",
            "Epoch 27/100, Loss: 0.6994\n",
            "Epoch 28/100, Loss: 0.6933\n",
            "Epoch 29/100, Loss: 0.6922\n",
            "Epoch 30/100, Loss: 0.6897\n",
            "Epoch 31/100, Loss: 0.6916\n",
            "Epoch 32/100, Loss: 0.6893\n",
            "Epoch 33/100, Loss: 0.6891\n",
            "Epoch 34/100, Loss: 0.6850\n",
            "Epoch 35/100, Loss: 0.6895\n",
            "Epoch 36/100, Loss: 0.6853\n",
            "Epoch 37/100, Loss: 0.6864\n",
            "Epoch 38/100, Loss: 0.6774\n",
            "Epoch 39/100, Loss: 0.6821\n",
            "Epoch 40/100, Loss: 0.6816\n",
            "Epoch 41/100, Loss: 0.6799\n",
            "Epoch 42/100, Loss: 0.6816\n",
            "Epoch 43/100, Loss: 0.6739\n",
            "Epoch 44/100, Loss: 0.6778\n",
            "Epoch 45/100, Loss: 0.6738\n",
            "Epoch 46/100, Loss: 0.6738\n",
            "Epoch 47/100, Loss: 0.6748\n",
            "Epoch 48/100, Loss: 0.6736\n",
            "Epoch 49/100, Loss: 0.6751\n",
            "Epoch 50/100, Loss: 0.6732\n",
            "Epoch 51/100, Loss: 0.6750\n",
            "Epoch 52/100, Loss: 0.6730\n",
            "Epoch 53/100, Loss: 0.6730\n",
            "Epoch 54/100, Loss: 0.6725\n",
            "Epoch 55/100, Loss: 0.6696\n",
            "Epoch 56/100, Loss: 0.6703\n",
            "Epoch 57/100, Loss: 0.6690\n",
            "Epoch 58/100, Loss: 0.6690\n",
            "Epoch 59/100, Loss: 0.6684\n",
            "Epoch 60/100, Loss: 0.6740\n",
            "Epoch 61/100, Loss: 0.6657\n",
            "Epoch 62/100, Loss: 0.6740\n",
            "Epoch 63/100, Loss: 0.6677\n",
            "Epoch 64/100, Loss: 0.6724\n",
            "Epoch 65/100, Loss: 0.6689\n",
            "Epoch 66/100, Loss: 0.6707\n",
            "Epoch 67/100, Loss: 0.6686\n",
            "Epoch 68/100, Loss: 0.6668\n",
            "Epoch 69/100, Loss: 0.6647\n",
            "Epoch 70/100, Loss: 0.6641\n",
            "Epoch 71/100, Loss: 0.6665\n",
            "Epoch 72/100, Loss: 0.6674\n",
            "Epoch 73/100, Loss: 0.6671\n",
            "Epoch 74/100, Loss: 0.6655\n",
            "Epoch 75/100, Loss: 0.6631\n",
            "Epoch 76/100, Loss: 0.6706\n",
            "Epoch 77/100, Loss: 0.6665\n",
            "Epoch 78/100, Loss: 0.6624\n",
            "Epoch 79/100, Loss: 0.6635\n",
            "Epoch 80/100, Loss: 0.6706\n",
            "Epoch 81/100, Loss: 0.6650\n",
            "Epoch 82/100, Loss: 0.6606\n",
            "Epoch 83/100, Loss: 0.6668\n",
            "Epoch 84/100, Loss: 0.6620\n",
            "Epoch 85/100, Loss: 0.6611\n",
            "Epoch 86/100, Loss: 0.6647\n",
            "Epoch 87/100, Loss: 0.6632\n",
            "Epoch 88/100, Loss: 0.6616\n",
            "Epoch 89/100, Loss: 0.6604\n",
            "Epoch 90/100, Loss: 0.6630\n",
            "Epoch 91/100, Loss: 0.6601\n",
            "Epoch 92/100, Loss: 0.6536\n",
            "Epoch 93/100, Loss: 0.6666\n",
            "Epoch 94/100, Loss: 0.6610\n",
            "Epoch 95/100, Loss: 0.6604\n",
            "Epoch 96/100, Loss: 0.6625\n",
            "Epoch 97/100, Loss: 0.6629\n",
            "Epoch 98/100, Loss: 0.6557\n",
            "Epoch 99/100, Loss: 0.6626\n",
            "Epoch 100/100, Loss: 0.6589\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es4-erMf3ChO"
      },
      "source": [
        "#Problem 3:\n",
        "\n",
        "Extend your code from problem 2 to support a single layer neural network with n hidden units (e.g. An  architecture: 784 > 10 > 10). These hidden units should be using sigmoid activations.\n",
        "\n",
        "For this problem you must write a function that takes as an input a matrix of x values, a list of y  values (as returned from problem 1), list of weight matrices, a list of bias vectors, and a learning rate and performs a single step of backpropagation. You will need to do both a forward step with the inputs to get the outputs, and then a backward prop to get the gradients. Return the  updated weight matrix and bias in the same format as it was passed.\n",
        "\n",
        "The list of weight matrices is a list with 2 entries where each entry in the list contains a single weight matrix as previously defined in problem 2. For a network with shape 784 > 10 > 10 the passed list  of weight matrices would look like this: [matrix with shape 784x10, matrix with shape 10x10]. Note:  though a hidden layer of size 10 is used as an example here, your code must be able to support a hidden  layer of dimension n.\n",
        "\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same  length as the first set of weights. (e.g. For an architecture of 784 > 10 > 10, there will be a two element  list with an vector of size 10 and a vector of size 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network2(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])] #xavier initialization\n",
        "\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    def gradient_descent(self, x, y, learning_rate):\n",
        "        # Forward pass\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activations[-1]) + b\n",
        "            zs.append(z)\n",
        "            activations.append(sigmoid(z))\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b = [delta]\n",
        "        nabla_w = [np.dot(delta, activations[-2].transpose())]\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
        "            nabla_b.append(delta)\n",
        "            nabla_w.append(np.dot(delta, activations[-l - 1].transpose()))\n",
        "        nabla_b.reverse()\n",
        "        nabla_w.reverse()\n",
        "\n",
        "        # Gradient descent update\n",
        "        self.weights = [w - learning_rate * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - learning_rate * nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return output_activations - y\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n"
      ],
      "metadata": {
        "id": "CTuHPiMSBqeq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Network2([784,10,10])"
      ],
      "metadata": {
        "id": "od3ASArjLuc9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "x, y = read_image_data('/content/drive/MyDrive/mnist_train.csv')\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "# (80% training, 20% test)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x_train = (x_train - np.mean(x_train)) / np.std(x_train)\n",
        "x_test = (x_test - np.mean(x_test)) / np.std(x_test)"
      ],
      "metadata": {
        "id": "aj2z1hxUL3Ag"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def train_model(net2, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "\n",
        "            # Perform gradient descent update\n",
        "            net2.gradient_descent(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "train_model(net2, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(net2, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = net2.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "y_pred = evaluate_model(net2, x_test, y_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3IWfcLhLqel",
        "outputId": "a6cf08b1-aaa9-47c9-ed4b-805ed8f88af0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   From problem 3 and 4, Experimenting with Xavier intialization {self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])]} helped neural netwrok with n hidden layers to increase the accuracy at some extent.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0AYlDKXzAA_r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT77_c2Q3IKw"
      },
      "source": [
        "#Problem 4:\n",
        "\n",
        "Extend your code from problem 3 (use cross entropy error) and implement a multi-layer neural  network, starting with a simple architecture containing any number of hidden units in each layer (e.g. With  architecture: 784 > 10 > 10 > 10). These hidden units should be using sigmoid activations.\n",
        "\n",
        "For this problem you must write a function that takes as an input a matrix of x values, a list of y  values (as returned from problem 1), list of weight matrices, a list of bias vectors, and a learning rate and  performs a single step of backpropagation. You will need to do both a forward step with the inputs to  get the outputs, and then a backward prop to get the gradients. Return the updated weight matrix and  bias in the same format as it was passed.\n",
        "\n",
        "The list of weight matrices is a list with k entries where each entry in the list contains a single  weight matrix as previously defined in problem 2. For a network with shape 784 > 10 > 10 > 10 the  passed list of weight matrices would look like this: [matrix with shape 784x10, matrix with shape 10x10,  matrix with shape 10x10]. Note: though a hidden layer of size 10 is used as an example here, your code  must be able to support a hidden layer of dimension n.\n",
        "\n",
        "The list of bias vectors will be in the form where each entry in the list is a vector with the same  length as the first set of weights. (e.g. For an architecture of 784 > 10 > 10, there will be a two element  list with an vector of size 10 and a vector of size 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MzxogZ2z3NJ8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Network3(object):\n",
        "    def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        #self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])] #xavier initialization\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    def gradient_descent3(self, x, y, learning_rate):\n",
        "        # Forward pass\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y)\n",
        "        nabla_b = [np.sum(delta, axis=1, keepdims=True)]\n",
        "        nabla_w = [np.dot(delta, activations[-2].transpose())]\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
        "            nabla_b.insert(0, np.sum(delta, axis=1, keepdims=True))\n",
        "            nabla_w.insert(0, np.dot(delta, activations[-l - 1].transpose()))\n",
        "\n",
        "        # Gradient descent update\n",
        "        self.weights = [w - learning_rate * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - learning_rate * nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations - y)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net3 = Network3([784,10,10,10])"
      ],
      "metadata": {
        "id": "Amf91fH1s9TZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "x, y = read_image_data('/content/drive/MyDrive/mnist_train.csv')\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the data into training and test sets (80% training, 20% test)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "x_train = (x_train - np.mean(x_train)) / np.std(x_train)\n",
        "x_test = (x_test - np.mean(x_test)) / np.std(x_test)"
      ],
      "metadata": {
        "id": "nr1ARIIotGcK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def train_model(net3, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "\n",
        "            net3.gradient_descent3(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "train_model(net3, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(net3, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = net3.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "\n",
        "y_pred = evaluate_model(net3, x_test, y_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FXxE9xStLV2",
        "outputId": "7088b253-5d22-4531-b4b0-4533546379c3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Multi-layered neural network and neural network with n hidden layers shows an increased accuracy while performing xavier initialization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q2tr6hVtB1Kz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GJyBnr3Nhd"
      },
      "source": [
        "#Problem 5:\n",
        "\n",
        "Extend your code from problem 4 to implement different activations functions which will be  passed as a parameter. In this problem all activations (except the final layer which should remain a  softmax) must be changed to the passed activation function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network4(object):\n",
        "    def __init__(self, sizes, activation_function):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])] # Xavier initialization\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
        "            a = self.activation_function(np.dot(w, a) + b)\n",
        "        z_output = np.dot(self.weights[-1], a) + self.biases[-1]\n",
        "        output = softmax(z_output)\n",
        "        return output\n",
        "\n",
        "    def gradient_descent4(self, x, y, learning_rate):\n",
        "        # Forward pass\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = self.activation_function(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y)\n",
        "        nabla_b = [np.sum(delta, axis=1, keepdims=True)]\n",
        "        nabla_w = [np.dot(delta, activations[-2].transpose())]\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = self.activation_prime(z)\n",
        "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
        "            nabla_b.insert(0, np.sum(delta, axis=1, keepdims=True))\n",
        "            nabla_w.insert(0, np.dot(delta, activations[-l - 1].transpose()))\n",
        "\n",
        "        # Gradient descent update\n",
        "        self.weights = [w - learning_rate * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - learning_rate * nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations - y)\n",
        "\n",
        "    def activation_prime(self, z):\n",
        "        if self.activation_function == sigmoid:\n",
        "            return sigmoid_prime(z)\n",
        "        elif self.activation_function == relu:\n",
        "            return relu_prime(z)\n",
        "        elif self.activation_function == tanh:\n",
        "            return tanh_prime(z)\n",
        "        elif self.activation_function == leaky_relu:\n",
        "            return leaky_relu_prime(z)\n",
        "\n",
        "# Define activation functions\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_prime(z):\n",
        "    return np.where(z > 0, 1, 0)\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def tanh_prime(z):\n",
        "    return 1 - np.tanh(z)**2\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.where(z > 0, z, z * alpha)\n",
        "\n",
        "def leaky_relu_prime(z, alpha=0.01):\n",
        "    return np.where(z > 0, 1, alpha)\n",
        "\n",
        "def softmax(z):\n",
        "    max_z = np.max(z)\n",
        "    exp_scores = np.exp(z - max_z)\n",
        "    return exp_scores / np.sum(exp_scores)\n"
      ],
      "metadata": {
        "id": "9er8tJd8jOK6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Here it is necessary to use Xavier initialization to get a proper accuracy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iuLrVZusEmkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Leaky ReLu"
      ],
      "metadata": {
        "id": "COKQvhWsjv0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [784, 10, 10, 10]\n",
        "activation_function = leaky_relu\n",
        "netlr = Network4(sizes, activation_function)"
      ],
      "metadata": {
        "id": "-1bEefASjsnl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "def train_model(netlr, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "            netlr.gradient_descent4(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "#hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "train_model(netlr, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(netlr, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = netlr.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "y_pred = evaluate_model(netlr, x_test, y_test)\n",
        "\n",
        "# accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGUyaE0dju-s",
        "outputId": "c278b79d-6a39-4486-a92a-15eb20b9ace8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9184166666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sigmoid\n"
      ],
      "metadata": {
        "id": "OxhK0WSuVVro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [784, 10, 10, 10]\n",
        "activation_function = sigmoid\n",
        "nets = Network4(sizes, activation_function)"
      ],
      "metadata": {
        "id": "uG5zld3oRlEM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "def train_model(nets, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "            nets.gradient_descent4(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "train_model(nets, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(nets, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = nets.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "y_pred = evaluate_model(nets, x_test, y_test)\n",
        "\n",
        "# accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7910c54-19cc-47e7-d459-0bf454a4239c",
        "id": "x8gFoms6Rbga"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.90725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ReLu\n"
      ],
      "metadata": {
        "id": "o1mXC1a_Vqj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [784, 10, 10, 10]\n",
        "activation_function = relu\n",
        "netr = Network4(sizes, activation_function)"
      ],
      "metadata": {
        "id": "iR0WcibtVqAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "def train_model(netr, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "            netr.gradient_descent4(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "\n",
        "# Train the model\n",
        "train_model(netr, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(netr, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = netr.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "y_pred = evaluate_model(netr, x_test, y_test)\n",
        "\n",
        "# accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gAJgAWXVzYQ",
        "outputId": "499ad118-6b50-480b-80b4-83fd4978d0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8941666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tanh"
      ],
      "metadata": {
        "id": "4P8LTdStWB-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [784, 10, 10, 10]\n",
        "activation_function = tanh\n",
        "nett = Network4(sizes, activation_function)"
      ],
      "metadata": {
        "id": "F-p-XPc7WDsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "def train_model(nett, x_train, y_train, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "            nett.gradient_descent4(x.reshape(-1, 1), y_one_hot, learning_rate)\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "\n",
        "# Train the model\n",
        "train_model(nett, x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "def evaluate_model(nett, x_test, y_test):\n",
        "    predictions = []\n",
        "    for x, y in zip(x_test, y_test):\n",
        "        # Forward pass\n",
        "        output = nett.feedforward(x.reshape(-1, 1))\n",
        "        prediction = np.argmax(output)\n",
        "        predictions.append(prediction)\n",
        "    return predictions\n",
        "y_pred = evaluate_model(nett, x_test, y_test)\n",
        "\n",
        "# accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFigkEGAWF2e",
        "outputId": "80a94890-8cd1-4d96-f84d-2d76c7dd9397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7325833333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   In problem 5, it is better to use Leaky ReLu. It gave accuracy of around 92% which is greter than sigmoid, ReLu and tanh\n",
        "\n"
      ],
      "metadata": {
        "id": "qY0c6FGSFby4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klegSin73SBF"
      },
      "source": [
        "#Problem 6:\n",
        "\n",
        "Extend your code from problem 5 to implement momentum with your gradient descent. The  momentum value will be passed as a parameter. Your function should perform epoch number of  epochs and return the resulting weights."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkM(object):\n",
        "    def __init__(self, sizes, activation_function):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])] # Xavier initialization\n",
        "        self.activation_function = activation_function\n",
        "        self.prev_delta_w = [np.zeros((y, x)) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "        self.prev_delta_b = [np.zeros((y, 1)) for y in sizes[1:]]\n",
        "\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
        "            a = self.activation_function(np.dot(w, a) + b)\n",
        "        z_output = np.dot(self.weights[-1], a) + self.biases[-1]\n",
        "        output = softmax(z_output)\n",
        "        return output\n",
        "\n",
        "    def gradient_descentM(self, x, y, learning_rate, momentum):\n",
        "        # Forward pass\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = self.activation_function(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y)\n",
        "        nabla_b = [np.sum(delta, axis=1, keepdims=True)]\n",
        "        nabla_w = [np.dot(delta, activations[-2].transpose())]\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = self.activation_prime(z)\n",
        "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
        "            nabla_b.insert(0, np.sum(delta, axis=1, keepdims=True))\n",
        "            nabla_w.insert(0, np.dot(delta, activations[-l - 1].transpose()))\n",
        "\n",
        "        # Gradient descent update with momentum\n",
        "        self.weights = [w - learning_rate * nw - momentum * pdw for w, nw, pdw in zip(self.weights, nabla_w, self.prev_delta_w)]\n",
        "        self.biases = [b - learning_rate * nb - momentum * pdb for b, nb, pdb in zip(self.biases, nabla_b, self.prev_delta_b)]\n",
        "        self.prev_delta_w = nabla_w\n",
        "        self.prev_delta_b = nabla_b\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations - y)\n",
        "\n",
        "    def activation_prime(self, z):\n",
        "        if self.activation_function == sigmoid:\n",
        "            return sigmoid_prime(z)\n",
        "        elif self.activation_function == relu:\n",
        "            return relu_prime(z)\n",
        "        elif self.activation_function == tanh:\n",
        "            return tanh_prime(z)\n",
        "        elif self.activation_function == leaky_relu:\n",
        "            return leaky_relu_prime(z)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-np.clip(z, -709, 709)))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_prime(z):\n",
        "    return np.where(z > 0, 1, 0)\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def tanh_prime(z):\n",
        "    return 1 - np.tanh(z)**2\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.where(z > 0, z, z * alpha)\n",
        "\n",
        "def leaky_relu_prime(z, alpha=0.01):\n",
        "    return np.where(z > 0, 1, alpha)\n",
        "\n",
        "def softmax(z):\n",
        "    max_z = np.max(z)\n",
        "    exp_scores = np.exp(z - max_z)  # Softmax normalization\n",
        "    return exp_scores / np.sum(exp_scores)\n"
      ],
      "metadata": {
        "id": "qgCR7BGZSvBB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sigmoid\n"
      ],
      "metadata": {
        "id": "neF9O1brHOYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [784, 10, 10, 10]\n",
        "activation_function = sigmoid\n",
        "netM = NetworkM(sizes, activation_function)\n",
        "\n",
        "# Train the model with momentum\n",
        "def train_model_with_momentum(netM, x_train, y_train, learning_rate, momentum, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            y_one_hot = np.zeros((10, 1))\n",
        "            y_one_hot[y] = 1\n",
        "\n",
        "\n",
        "            netM.gradient_descentM(x.reshape(-1, 1), y_one_hot, learning_rate, momentum)\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = .001\n",
        "momentum = 0.1\n",
        "epochs = 100\n",
        "\n",
        "# Train the model\n",
        "train_model_with_momentum(netM, x_train, y_train, learning_rate, momentum, epochs)\n",
        "\n",
        "# Evaluate the trained model\n",
        "y_pred = evaluate_model(netM, x_test, y_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "QimDqrXIS9RA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fce16f8-bd08-41cf-f84e-f77443647c5c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.89575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation function: Sigmoid\n",
        "\n",
        "*   For Learning_rate=0.01, momentum =0.1 and epochs=10: Accuracy = 85%\n",
        "*   learning_rate = 0.001, momentum = 0.01 and epochs = 10: Accuracy = 90%\n",
        "*   learning_rate = 0.0001, momentum = 0.01 and epochs = 10: Accuracy = 90.4%\n",
        "*   learning_rate = 0.0001, momentum = 0.001 and epochs = 10: Accuracy = 90.2%\n",
        "*   learning_rate = 0.001, momentum = 0.2 and epochs = 10: Accuracy = 82.8%\n",
        "*   learning_rate = 0.001, momentum = 0.05 and epochs = 10: Accuracy = 90.2%\n",
        "*   learning_rate = 0.0001, momentum = 0.005 and epochs = 10: Accuracy = 91.2%\n",
        "*   learning_rate = 0.001, momentum = 0.1 and epochs = 100: Accuracy = 89.5%\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vg-VDX4AHePi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}